{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.BERT Fine-tuning for Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
    "# pip install transformers\n",
    "# pip install nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from nlp import load_dataset\n",
    "import numpy as np\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11_M4ootuT7I1G0RlihcC0cA3Elqotlc-\n",
      "To: /Users/leesanghyuk/Python_Programs/BERT/imdb.csv\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 132k/132k [00:00<00:00, 630kB/s]\n",
      "Using custom data configuration default\n"
     ]
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú\n",
    "url = 'https://drive.google.com/uc?id=11_M4ootuT7I1G0RlihcC0cA3Elqotlc-'\n",
    "output_path = '/Users/leesanghyuk/Python_Programs/BERT/imdb.csv'\n",
    "gdown.download(url, output_path, quiet=False)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "dataset = load_dataset('csv', data_files=output_path, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlp.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ ÌôïÏù∏\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÌïôÏäµ/ÌÖåÏä§Ìä∏Î°ú Î∂ÑÌï†\n",
    "dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset(features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}, num_rows: 70),\n",
       " 'test': Dataset(features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}, num_rows: 30)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÌôïÏù∏\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±\n",
    "train_data = dataset['train']\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Îç∞Ïù¥ÌÑ∞Îäî Îã§Ïö¥ Î∞è Î∂ÑÌï†Ïù¥ ÏôÑÎ£åÎêòÏóàÎã§. Ïù¥Ï†ú BERT Î™®Îç∏ÏùÑ Îã§Ïö¥ Î∞õÍ≥† ÏÇ¨Ïö©ÌïòÏûê."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ïù¥Ï†ú Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨Î•º ÏßÑÌñâÌïúÎã§. Îã§ÏùåÏùò Îã®Í≥ÑÎ•º ÏàòÎèôÏúºÎ°ú Ìï¥Ï§Ñ Ïàò ÏûàÏßÄÎßå, BERT ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º ÏÇ¨Ïö©ÌïòÎ©¥ ÏûêÎèôÏúºÎ°ú Ìï¥Ï§ÄÎã§.\n",
    "- ÌÜ†ÌÅ∞Ìôî\n",
    "- ÌäπÏàò ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä\n",
    "- IDÎ°ú Î≥ÄÌôò\n",
    "- ÏÑ∏Í∑∏Î®ºÌä∏ ID ÏÉùÏÑ±\n",
    "- Ïñ¥ÌÖêÏÖò ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 3000, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÇ¨Ïö© ÏòàÏãú\n",
    "tokenizer('I love Paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leesanghyuk/anaconda3/envs/PySang/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2663: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2293, 3000, 102, 0], [101, 2017, 2066, 10140, 3899, 102], [101, 4586, 2991, 102, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÏûêÎèôÏúºÎ°ú Ìå®Îî©ÏùÑ ÏàòÌñâÌïòÎäî ÏòàÏãú\n",
    "tokenizer(['I love Paris', 'You like cute dog', 'snow fall'], padding=True, max_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```max_length```Í∞Ä 5Ïù¥Í∏∞ ÎïåÎ¨∏Ïóê ÌÜ†ÌÅ∞ Í∞úÏàòÍ∞Ä 5Í∞úÎ≥¥Îã§ Ï†ÅÏùÄ Î¨∏Ïû•ÏùÄ Ìå®Îî©Ïù¥ Ï†ÅÏö©ÎêòÏóàÎã§.\n",
    "- Ìå®Îî©Ïù¥ ÏßÑÌñâÎêú Î¨∏Ïû•ÏùÄ ```attention_mask```Í∞Ä 1Ïù∏ Î∂ÄÎ∂ÑÏù¥ ÏûàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ÑÏ≤òÎ¶¨Î•º ÏúÑÌïú Ìï®Ïàò\n",
    "def preprocess(data):\n",
    "    return tokenizer(data['text'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏóêÎü¨ Ìï¥Í≤∞ÏùÑ ÏúÑÌï¥ Ìå®ÌÇ§ÏßÄ ÏóÖÎç∞Ïù¥Ìä∏\n",
    "# pip install dill==0.3.5.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676bfc87ddce43e39045f45aa82b6a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2617e28a461249e29d792032dbdc1ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö©\n",
    "train_data = train_data.map(preprocess, batched=True, batch_size=len(train_data))\n",
    "test_data = test_data.map(preprocess, batched=True, batch_size=len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Í≥ÑÏÜçÌï¥ÏÑú ÏóêÎü¨Í∞Ä Î∞úÏÉùÌñàÎã§.\n",
    "- PY3ÏôÄ Í¥ÄÎ†®Îêú Î¨∏ÏÑúÎäî Ï∞æÏùÑ Ïàò ÏóÜÏñ¥ÏÑú ÎπÑÏä∑Ìïú ÏóêÎü¨Î•º Ïñ∏Í∏âÌïú Î¨∏ÏÑúÎ•º ÌôïÏù∏ ÌõÑ ÏãúÎèÑÌï¥Î¥§ÎçîÎãà Ìï¥Í≤∞ÎêêÎã§...üòÆ‚Äçüí®\n",
    "- [Ïä§ÌÉùÏò§Î≤ÑÌîåÎ°úÏö∞]('https://stackoverflow.com/questions/74857932/attributeerror-module-dill-dill-has-no-attribute-log')Î•º Ï∞∏Í≥†Ìï¥ÏÑú ```dill``` Î≤ÑÏ†ÑÏùÑ Ï°∞Ï†ïÌñàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞Ïùò ÌòïÏãù ÏßÄÏ†ï\n",
    "train_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ïù¥Î†áÍ≤å Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑÎ•º ÏôÑÎ£åÌñàÎã§. Î™®Îç∏ÏùÑ ÌïôÏäµÏãúÏºúÎ≥¥Ïûê."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï†ïÏùò\n",
    "batch_size = 8\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏõúÏóÖ, Í∞ÄÏ§ëÏπò ÎîîÏºÄÏù¥ ÏÑ§Ï†ï\n",
    "warmup_steps = 500\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïôÏäµÏùÑ ÏúÑÌïú Ïù∏Ïûê Ï†ïÏùò\n",
    "training_args = TrainingArguments(output_dir='./results', \n",
    "                                  num_train_epochs=epochs,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  warmup_steps=warmup_steps,\n",
    "                                  weight_decay=weight_decay,\n",
    "                                  logging_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ìä∏Î†àÏù¥ÎÑà Ï†ïÏùò\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=train_data,\n",
    "                  eval_dataset=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïôÏäµ\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î™®Îç∏ ÌèâÍ∞Ä\n",
    "#trainer.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

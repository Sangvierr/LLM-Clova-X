{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create BERT Embedding from all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델 불러오기\n",
    "model = BertModel.from_pretrained('bert-base-uncased', \n",
    "                                  output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bert-based-uncased를 사용한다. \n",
    "- 모든 토큰을 소문자화한 모델이다.\n",
    "- 임베딩 벡터의 크기는 768이다.\n",
    "- ```output_hidden_states=True```로 설정해서 모든 인코더 레이어에서 임베딩을 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 문장을 토큰화하고 ID를 얻는 전처리 과정을 진행하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'paris']\n"
     ]
    }
   ],
   "source": [
    "# 입력 문장\n",
    "sentence = 'I love Paris'\n",
    "\n",
    "# 문장 토큰화\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# 확인\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전 학습된 토크나이저가 문장을 토큰 단위로 분리한다.\n",
    "- 이번에는 특수 토큰을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'paris', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 특수 토큰 추가\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'paris', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# 패딩 추가\n",
    "tokens = tokens + ['[PAD]'] + ['[PAD]']\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 생성\n",
    "attn_mask = [0 if i == '[PAD]' else 1 for i in tokens]\n",
    "\n",
    "print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 3000, 102, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 토큰 ID 생성\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전 학습된 토크나이저가 토큰을 ID로 매핑했다.\n",
    "- 이제 ID와 어텐션 마스크를 텐서로 만들어주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2293, 3000,  102,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서화\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attn_mask = torch.tensor(attn_mask).unsqueeze(0)\n",
    "\n",
    "print(token_ids)\n",
    "print(attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임베딩을 추출해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 가져오기\n",
    "last_hidden_state, pooler_output, hidden_states = model(token_ids, attention_mask = attn_mask, return_dict = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model은 튜플 형태로 3가지 값을 반환한다.\n",
    "- ```last_hidden_state``` : 최종 인코더 계층(12번째 인코더)에서 얻은 표현\n",
    "- ```pooler_output``` : 최종 인코더 계층(12번째 인코더)의 [CLS] 토큰 표현\n",
    "- ```hidden_states``` : 모든 인코더 계층에서 얻은 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "😅 transformers 3.xx 버전에서는 튜플 형태를 명시적으로 출력해달라고 해야 한다. 따라서 ```return_dict=False```로 설정한다.\n",
    "- 참고 : [stackoverflow](https://stackoverflow.com/questions/66524542/attributeerror-str-object-has-no-attribute-shape-while-encoding-tensor-usin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "# 모든 토큰의 임베딩 표현 확인\n",
    "print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```last_hidden_state```는 각 토큰의 임베딩 벡터를 출력하게 된다.\n",
    "- [배치 크기, 시퀀스 길이, 은닉 상태 크기]를 나타낸다.\n",
    "- **배치 크기** : 여기서는 1이다. 만약 여러개 문장을 사용했다면, 배치 크기가 늘어나게 된다.\n",
    "- **시퀀스 길이** : 문장의 길이, 토큰의 개수를 나타낸다.\n",
    "- **은닉 상태 크기** : 임베딩 벡터의 크기로, bert-base 모델은 768이다.\n",
    "\n",
    "😁 마지막 계층의 은닉 상태를 얻는 부분이라 이전과 동일하다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 다음과 같이 각 토큰의 표현을 얻을 수 있다.\n",
    "- ```last_hidden_state[0][0]```은 첫번째 배치의 첫번째 시퀀스이므로 '[CLS]'의 임베딩 벡터이다.\n",
    "- ```last_hidden_state[0][1]```은 첫번째 배치의 두번째 시퀀스이므로 'I'의 임베딩 벡터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0410e+00,  7.7545e-01,  1.0335e+00, -2.5202e-01,  3.0039e-01,\n",
      "         8.8043e-03, -2.2426e-01,  2.6048e-01, -5.2928e-01, -4.6895e-01,\n",
      "        -4.2587e-01, -5.7644e-01, -4.5401e-01,  6.2828e-01, -1.0082e-01,\n",
      "         3.2514e-01,  3.8628e-01,  2.0141e-01,  4.4131e-01,  4.3412e-01,\n",
      "         3.7723e-01,  5.1090e-01, -3.7849e-01,  1.2689e-01,  1.2464e-01,\n",
      "        -7.2938e-01, -3.5591e-01, -8.4662e-02,  5.2243e-01,  8.9538e-02,\n",
      "         3.9896e-01,  6.2042e-01, -4.3549e-02, -1.3473e-01, -5.1585e-01,\n",
      "         3.8026e-01, -4.8698e-01,  2.8725e-01, -5.1415e-01,  2.4158e-01,\n",
      "         4.5858e-02, -1.1117e-01,  1.3997e-01, -3.4089e-01,  6.4368e-01,\n",
      "        -7.3179e-01,  2.6873e-01,  6.0686e-01,  1.8135e+00, -5.0584e-01,\n",
      "         1.0304e-01,  2.7961e-01, -5.5215e-02,  4.5086e-01, -6.9957e-02,\n",
      "         1.1098e+00, -8.3400e-01, -8.3123e-01, -2.0023e-01,  1.0537e-01,\n",
      "        -4.8093e-01, -7.0247e-01,  1.4388e-01, -3.7023e-01, -2.4109e-01,\n",
      "         6.4811e-01, -3.5716e-01,  2.9457e-01, -6.2665e-01, -1.1939e+00,\n",
      "        -3.6355e-01, -4.8936e-01,  1.0619e+00, -1.7914e-01, -6.2232e-01,\n",
      "        -1.2752e-01, -8.6891e-01,  6.3909e-01,  1.3171e-01, -1.4308e-01,\n",
      "        -6.7594e-01,  1.0558e+00,  2.4046e-03,  1.0994e+00,  4.1803e-01,\n",
      "        -5.6364e-02, -6.6181e-02,  3.5943e-01, -4.2058e-02,  1.5793e-01,\n",
      "        -4.3695e-01, -4.3810e-01, -3.4055e-01,  4.1865e-01,  1.6490e-01,\n",
      "        -4.7196e-01, -1.9621e-01,  7.9834e-01, -7.5696e-01, -4.8508e-01,\n",
      "         8.4592e-02,  1.9896e-01,  6.7979e-01, -3.8288e-02, -5.3837e-01,\n",
      "        -2.1036e-01,  8.0734e-02,  3.6126e-01,  1.5469e+00,  2.4523e-01,\n",
      "         4.9221e-01, -8.9900e-01, -6.8508e-02, -1.3293e-01, -6.7194e-01,\n",
      "         3.9446e-02, -1.7960e-01, -4.1208e-02, -7.0522e-01, -1.4858e-01,\n",
      "        -3.2784e-01,  9.6860e-02, -3.9851e-01,  2.8753e-01,  1.5583e-01,\n",
      "         2.5173e-01, -9.9568e-01,  5.4914e-01, -1.4008e-01, -4.3337e-01,\n",
      "        -1.2714e-01,  4.6426e-01, -2.3458e-01, -5.0520e-01,  5.7172e-01,\n",
      "         3.7620e-01,  9.9764e-01, -6.8278e-01, -1.9783e-01,  2.3377e-01,\n",
      "         3.9707e-01,  6.6745e-01,  4.9547e-01,  5.3270e-01,  6.9622e-01,\n",
      "         4.0326e-01, -4.5125e-01, -4.2264e-01, -3.7949e-02,  9.8824e-02,\n",
      "         6.5381e-01,  4.2996e-02,  4.2540e-01,  7.6359e-01, -6.4306e-01,\n",
      "        -8.5945e-01, -3.9223e-01,  5.8867e-01,  5.0033e-01,  1.9020e-01,\n",
      "        -1.1731e-01, -2.2902e-01,  2.8608e-01,  9.5602e-01, -3.1967e-01,\n",
      "         3.7882e-01,  2.0140e-01,  3.1487e-01, -5.7583e-01, -2.8939e-01,\n",
      "         6.4281e-02,  3.4125e-01,  9.8716e-01,  4.8837e-01,  2.5502e-01,\n",
      "         2.8994e-01,  1.2712e-01,  3.2370e-01,  7.1778e-02, -5.3239e-02,\n",
      "        -2.5916e-01,  8.3798e-01, -5.7880e-01, -3.4529e-01,  3.9566e-01,\n",
      "         3.8473e-02,  5.7517e-01, -1.1120e+00, -6.8480e-02,  2.9661e-01,\n",
      "         4.2437e-02,  6.0736e-01, -1.1352e+00,  1.8857e-02,  2.4926e-01,\n",
      "        -5.3988e-01,  9.0241e-02, -4.8626e-01,  5.4401e-01,  5.6300e-01,\n",
      "         2.8760e-02, -2.3715e-01,  7.0000e-01,  2.6858e-01, -3.8745e-01,\n",
      "        -5.8358e-01,  1.6707e-01,  5.2816e-01, -3.1621e-02,  1.8172e-01,\n",
      "        -2.0221e-01,  3.1806e-01,  2.8452e-01,  4.4809e-01, -8.9432e-03,\n",
      "         5.8008e-02,  3.8719e-01, -3.5259e-01,  4.5368e-01,  5.4674e-01,\n",
      "         6.6151e-02, -2.8956e-01, -8.7650e-01,  7.0197e-01, -1.6741e-01,\n",
      "         3.8276e-01, -1.4576e-01, -7.1218e-01, -2.0263e-01,  1.2013e+00,\n",
      "        -1.9592e-01, -5.1199e-01,  2.8101e-01,  3.0106e-01,  8.3517e-02,\n",
      "         6.8690e-01, -2.7221e-01,  1.4052e-01, -4.4821e-02,  1.3271e-02,\n",
      "         1.2463e-01, -9.9017e-01,  8.1798e-01,  6.3769e-01, -3.0494e-01,\n",
      "        -1.3994e-01, -4.3987e-01, -7.0567e-01, -5.6005e-01,  1.9635e-02,\n",
      "        -1.5162e-01, -1.5796e+00, -1.5200e-01, -4.5896e-01, -1.0417e+00,\n",
      "        -4.4309e-01,  4.0796e-01,  4.1747e-01, -3.4027e-01,  8.1738e-02,\n",
      "        -6.5190e-01, -1.9215e-01,  1.2135e+00,  2.4938e-01, -6.1509e-01,\n",
      "        -8.0404e-01, -1.0522e+00, -3.5428e-02,  2.7185e-01, -3.9405e-01,\n",
      "        -2.8596e-01,  2.8000e-02,  4.4214e-01,  1.2790e+00,  3.7079e-01,\n",
      "        -3.7343e-01,  6.8154e-01,  3.7910e-01,  5.3238e-01, -6.4829e-01,\n",
      "         1.5928e-02,  8.9113e-01, -6.1637e-01, -4.5423e-01,  7.4852e-01,\n",
      "         1.3326e-01, -5.1139e-01, -3.2529e-01, -7.9913e-01, -5.9469e-01,\n",
      "        -2.5839e-01,  3.1668e-01,  9.9075e-02, -5.6504e-01, -3.1001e-01,\n",
      "         1.5510e-01,  1.7886e-01,  4.0448e-01,  4.5598e-02, -2.5275e-02,\n",
      "        -3.6845e-01,  3.9325e-01, -8.4171e-01,  3.7807e-01, -2.1526e-01,\n",
      "         2.3252e-01,  5.2708e-02,  7.0497e-02, -4.2760e+00,  2.7845e-01,\n",
      "         1.3654e-01, -2.6253e-01,  1.2231e+00,  2.6599e-01,  3.4658e-02,\n",
      "        -1.2048e-01, -5.7294e-02, -1.9830e-01,  4.8341e-01, -1.6869e-01,\n",
      "         3.2518e-01,  4.5720e-01,  3.8839e-01, -1.9749e-01, -2.2866e-01,\n",
      "        -9.3246e-01, -3.4850e-01,  9.4926e-01, -2.4100e-01,  6.0417e-01,\n",
      "         4.9146e-01, -9.7551e-01,  5.8243e-01,  1.6170e+00,  1.6788e-01,\n",
      "         1.0873e-01, -1.1330e+00, -2.3490e-01, -5.0805e-01, -5.2035e-01,\n",
      "         5.3383e-01,  4.3514e-01, -3.3844e-02, -4.7684e-01,  2.2036e-01,\n",
      "        -7.3453e-01, -6.7993e-01,  7.4598e-01, -6.0477e-01, -4.7275e-01,\n",
      "         3.2420e-01, -7.2775e-01,  2.4810e-01, -3.8313e-01, -1.1920e-01,\n",
      "        -3.5554e-01, -8.0616e-03,  1.0294e+00, -1.3204e-02,  3.4899e-01,\n",
      "         4.2123e-01, -4.9294e-01, -2.5757e-01, -2.6447e-01,  2.2005e-01,\n",
      "         4.8371e-01, -6.9707e-02, -2.4341e-01,  4.1554e-01,  4.2987e-01,\n",
      "        -5.9623e-01,  2.5713e-01,  5.4961e-01, -6.7237e-01, -2.0433e-01,\n",
      "        -9.5195e-01, -7.0805e-01,  8.4126e-01, -2.0036e-01,  4.5977e-01,\n",
      "        -3.1604e-02, -9.6901e-01, -1.4022e+00, -3.9701e-01, -1.2748e-01,\n",
      "         4.0215e-01, -3.7840e-01, -3.6796e-01, -9.4802e-01, -3.3611e-01,\n",
      "        -3.4957e-01,  4.8190e-01, -3.9909e-01, -9.3948e-01, -2.2648e-01,\n",
      "         9.1491e-03, -3.1424e-02,  5.5644e-02,  2.3262e-01, -9.0554e-02,\n",
      "         2.3298e-01,  2.6186e-01,  1.0101e+00,  1.6593e-03,  3.9107e-01,\n",
      "        -3.9313e-01,  3.4960e-01, -5.0343e-01,  1.0183e+00, -4.0011e-02,\n",
      "        -2.0227e-01,  3.6669e-01,  3.6230e-01,  5.2813e-02, -1.8624e-01,\n",
      "         6.5796e-01, -3.4857e-01,  4.9920e-01, -3.7103e-01, -3.9768e-01,\n",
      "        -2.2802e-01, -2.1782e-01, -5.9031e-01, -8.9766e-01,  3.9204e-02,\n",
      "         8.2192e-01, -2.0151e-01,  2.1152e-01,  1.0665e-01,  8.6927e-01,\n",
      "        -3.7575e-02, -3.0432e-01, -5.1943e-01, -3.9354e-02,  6.0594e-01,\n",
      "        -4.9006e-02, -1.1428e-01,  6.4861e-01, -3.9855e-01, -3.2385e-01,\n",
      "         6.2001e-01,  1.6643e-01, -2.5584e-02,  9.1290e-03, -4.0773e-01,\n",
      "        -5.6441e-01, -2.7629e-01,  6.8134e-01,  7.0636e-02,  4.0223e-03,\n",
      "        -6.5499e-01, -9.5685e-02, -4.3442e-01, -1.5711e-01, -2.4877e-01,\n",
      "        -2.3338e-01, -1.0562e-01, -2.8895e-01, -1.0276e+00, -2.1201e-01,\n",
      "         7.5371e-01, -3.1960e-01,  1.3395e-02, -7.0094e-01,  5.6762e-01,\n",
      "         2.2837e-01,  5.0091e-01, -9.7256e-01, -5.6920e-01, -7.5309e-01,\n",
      "        -2.5563e-01,  3.9968e-01,  1.2754e-01,  3.6545e-01,  3.9153e-01,\n",
      "        -8.5447e-01, -1.4664e-01,  9.3709e-02, -1.2236e-01, -2.7470e-01,\n",
      "        -3.8278e-01, -3.8676e-02, -1.1936e-01,  6.4504e-01, -3.1873e-02,\n",
      "        -8.1422e-01,  3.6069e-01,  1.0932e-01, -5.8253e-02, -3.4286e-01,\n",
      "        -5.3895e-01, -1.6123e-01,  4.7909e-01, -6.1627e-02,  6.3737e-01,\n",
      "        -4.1572e-01,  8.2939e-02,  6.6788e-01, -6.0385e-01,  5.5138e-01,\n",
      "        -1.0060e+00, -4.2594e-01, -1.3688e+00, -6.9770e-01,  2.0765e-01,\n",
      "        -5.3150e-01,  2.9374e-01,  7.9149e-01,  1.0866e-01, -2.6733e-01,\n",
      "        -7.2754e-02,  2.6555e-01,  5.1118e-01, -4.4613e-02,  2.6499e-01,\n",
      "         3.7843e-01,  4.5865e-02,  3.9101e-01, -5.0652e-01, -8.6298e-01,\n",
      "         1.3862e-01,  5.1206e-02,  2.1927e-01,  6.4775e-01,  3.3486e-02,\n",
      "        -1.6130e-01, -3.6630e-01, -4.1122e-01, -3.3926e-02,  2.2110e-01,\n",
      "         1.4691e-01,  3.0185e-01, -8.4018e-01, -3.1332e-01, -5.1478e-01,\n",
      "        -3.0653e-01, -4.2094e-01, -2.1337e-01,  3.4276e-01, -8.0348e-01,\n",
      "         8.1351e-01, -8.8322e-01,  1.8754e-01, -4.2430e-03, -2.0427e-01,\n",
      "        -5.1065e-01, -2.8259e-01, -1.5977e-01,  1.9840e-01, -8.6513e-01,\n",
      "        -1.0693e-03, -1.2832e-01,  5.5990e-01,  1.1191e+00, -4.1539e-01,\n",
      "         3.8717e-01, -9.8178e-02, -1.0800e-01,  1.2291e-01,  5.8803e-01,\n",
      "         1.1301e-01, -6.3511e-01,  6.1038e-01, -1.2902e+00, -4.8960e-01,\n",
      "         2.6672e-01,  1.4062e-01,  4.8861e-01, -7.9505e-02,  3.9473e-01,\n",
      "         9.8782e-01, -1.7699e-01, -1.9313e-01,  9.1936e-02,  5.7572e-01,\n",
      "        -3.3983e-01,  2.2481e-02,  2.7465e-01,  1.1754e-01,  4.0983e-01,\n",
      "         1.2994e-01, -2.8260e-01,  6.7121e-01, -3.8846e-02, -8.3327e-01,\n",
      "        -9.6713e-01,  5.6600e-01, -5.5009e-01, -3.5629e-01,  9.4874e-01,\n",
      "        -1.0099e+00, -5.6296e-01,  4.3981e-01, -3.4455e-01,  8.5906e-03,\n",
      "         8.2912e-01, -4.4527e-01,  8.6684e-01,  8.6470e-02, -4.5294e-02,\n",
      "         3.9480e-01,  3.3462e-01,  3.1254e-01,  1.0378e-01,  3.8430e-01,\n",
      "         1.7275e-01,  9.3130e-03,  2.6959e-01, -4.1051e-01,  3.6453e-01,\n",
      "         1.6913e-01, -1.5383e-01,  4.3060e-01,  2.6051e-01,  2.2257e-01,\n",
      "        -7.6702e-01, -1.6267e-01,  1.5156e+00, -1.9982e-01, -7.7821e-01,\n",
      "         6.4170e-01,  3.7870e-01, -6.8978e-01, -1.6163e-01,  3.5983e-01,\n",
      "        -4.9500e-01, -9.9714e-02, -6.1543e-03, -1.6397e-01, -4.9621e-01,\n",
      "         9.7835e-01,  4.4148e-01,  1.5251e-01,  8.1372e-01, -8.6836e-01,\n",
      "         7.3656e-02,  2.5901e-01, -1.9860e-02,  1.4429e-01, -1.1161e-01,\n",
      "        -1.7982e-01,  8.9053e-01,  2.0905e-01,  5.6438e-01,  5.7741e-01,\n",
      "        -7.8162e-01,  8.5271e-01,  1.0902e+00,  1.3509e-01, -3.3312e-01,\n",
      "         5.0567e-01,  9.1032e-01, -3.1855e-01, -6.2157e-01,  4.2668e-01,\n",
      "        -1.8931e-01,  9.0465e-01, -1.2046e+00,  7.1146e-02,  7.3532e-01,\n",
      "        -1.0485e-01, -4.4048e-01, -3.6647e-01,  1.1015e-01,  3.0159e-01,\n",
      "         1.2651e-01,  1.2801e+00,  3.6619e-01, -4.2970e-01, -6.0071e-01,\n",
      "         4.8227e-01, -2.7022e-02,  1.7986e-01,  4.7809e-02,  3.7751e-01,\n",
      "        -1.0219e+00, -1.2662e+00, -4.2298e-02, -1.0570e+00, -4.1428e-01,\n",
      "         7.2005e-01,  1.5541e-02, -6.5134e-02, -3.0862e-01, -4.7438e-01,\n",
      "        -8.3271e-01, -4.3007e-01,  9.2886e-02, -4.2217e-01,  1.3151e-01,\n",
      "         4.1800e-01,  3.0031e-01, -8.9493e-02, -7.3912e-01, -1.4035e-01,\n",
      "        -7.9948e-01,  1.6279e-01, -2.7325e-01,  6.2518e-01, -1.8292e-01,\n",
      "         1.9174e-01, -7.3314e-01,  9.4727e-02,  4.5690e-02, -6.3250e-01,\n",
      "         5.7075e-01, -2.5274e-02,  4.2116e-01,  2.7263e-02, -1.0826e+00,\n",
      "        -5.1750e-01,  3.5483e-02, -2.2691e-01,  3.0331e-01, -9.5368e-02,\n",
      "         4.9847e-01, -3.3918e-01, -2.1160e-02,  3.2187e-01,  7.8925e-01,\n",
      "        -8.3445e-01, -2.4255e-01, -6.3296e-02, -7.3961e-02,  8.0912e-01,\n",
      "        -3.4917e-01, -6.9803e-01, -1.9042e-01, -2.6729e-01,  1.9499e-01,\n",
      "         2.5265e-01,  1.1049e+00, -3.5245e-01,  1.3218e-01, -7.7170e-02,\n",
      "         4.1406e-01,  5.5323e-02, -1.3936e-01, -3.6627e-01,  6.5057e-01,\n",
      "         4.4747e-02,  4.4982e-01, -5.4020e-01, -7.7029e-01,  1.3346e-03,\n",
      "        -3.4674e-01, -6.6902e-01, -2.1543e-01,  2.4145e-02,  2.1370e-01,\n",
      "        -4.6173e-01,  1.8395e-01,  3.6621e-01,  2.5191e-01,  1.5981e-01,\n",
      "         7.4831e-02, -5.9887e-01, -3.5925e-01,  9.1384e-01, -5.5256e-01,\n",
      "        -2.3932e-01, -7.1845e-04,  6.5842e-01,  2.1419e-03,  2.5356e-01,\n",
      "        -5.6209e-01,  5.2182e-01, -8.5238e-02], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# love의 임베딩 벡터 확인\n",
    "print(last_hidden_state[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# CLS 토큰의 표현 확인\n",
    "print(pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```[CLS]``` 토큰의 값을 출력한다.\n",
    "- [배치 크기, 은닉 상태 크기]를 나타낸다.\n",
    "- ```[CLS]``` 토큰을 문장의 표현 벡터로 사용할 수 있다.\n",
    "\n",
    "😁 역시나 동일하게 ```[CLS]```의 표현을 얻는 부분이라 이전과 동일하다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 인코더 계층 확인\n",
    "len(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 임베딩 레이어($h_0$)에서 최종 인코더 레이어($h_{12}$)까지를 포함해서 13개의 값을 포함하는 튜플이다.\n",
    "- ```hidden_states[0]```은 입력 임베딩 레이어에서 얻은 모든 토큰의 표현 벡터\n",
    "- ```hidden_states[3]```은 세 번째 인코더 계층에서 얻은 모든 토큰의 표현 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 임베딩 레이어에서 얻은 모든 토큰의 표현 벡터이다.\n",
    "- [배치 크기, 시퀀스 길이, 은닉 상태 크기]를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states[3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3번째 인코더 레이어에서 얻은 모든 토큰의 표현 벡터이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
